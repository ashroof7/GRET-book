%----------------------------------------------------------------------------------------
%	CHAPTER 1 // Introduction
%----------------------------------------------------------------------------------------
\chapterimage{cover_1.jpg} 
\chapter{Introduction and Related Work}
\label{ch_intro}
\section{Introduction} \index{Introduction}
In this project we use two different HCI techniques; eye-tracking and gesture recognition. By using these techniques we aim to facilitate human computer interaction and provide a new way of accessibility for people with disabilities. Using Eye-tracking we detect the point of gaze which can be translated in some action. Gesture recognition is used to interpret hand gestures into simple actions like   capturing a photo. Not to mention that gesture recognition can provide a notable advantage to the deaf since it is similar to sign language. In general we the 2 methods we are serving can be used to enhance user experience interacting with computers, specially wearable computers. \bigskip

Through this book we will demonstrate 3 different eye tracking algorithms; Starburst \cite{starburst}, Pupil \cite{pupil}, and Mirt, in chapters \ref{ch_starburst}, \ref{ch_pupil} and \ref{ch_mirt} respectively. Then we discuss the performance of these algorithms in chapter \ref{ch_eye_tr_discussion}. In chapters \ref{ch_gesture} and \ref{ch_gr_eval} we discuss different approaches we use to recognize static and dynamic gestures. Finally we present our conclusion and future work in chapter \ref{ch_conclusion}.


\section{Motivation} \index{Motivation}
It is largely a software concern; when software, hardware, or a combination of hardware and software, is used to enable use of a computer by a person with a disability or impairment. Developing HCI techniques that ease interaction with computer has become a must. Traditional input methods like keyboards, mouses and even touch screens can be stumbling block in the way how persons with disability interact with computers. \bigskip

Looking from a different prescriptive, wearable technology lately has been a very promising field. Ubiquitous computers are expected to be an essential part of our lifestyle in the near future. Introducing new pervasive wearable devices (i.e Google Glass, smart watches) opens a new area for development. Not only such devices have  changed the way interact with computer, but also they leave the door open for new HCI techniques. For example using voice commands is one of the most popular methods to interact with wearable gadget. Despite the simplicity of using voice commands, it might not be acceptable in some environments like libraries. Hence developing new techniques that cope together to serve a new user experience has become essential. \bigskip

We believe that eye-tracking and gesture recognition based techniques can offer a new user experience and can ease the way people interact with wearable gadgets. Moreover, gesture recognition can be a very appealing way for the deaf to ease interaction with computers due to the similarity with sign language. 


\section{Related Work}\index{Related Work}
\subsection{Eye Tracking} \index{Eye Tracking}
Eye tracking is to collect data using either a remote or head-mounted 'eye tracker' connected to a computer. Most generally eye trackers include two common components: a light source and a camera. The light source(usually infra-red) is directed towards the eye. The camera tracks the reflection of the light source along with visible ocular features such as pupil. This data is usually used in estimating gaze of point. \bigskip

There are many different methods for exploring eye tracking data. The most common is to analyze the visual path of one or more participants across an interface such as a computer screen. So, the presence or absence of eye data points in different screen areas can be examined. This type of analysis is used to determine which features are seen, when a particular feature captures attention, how quickly the eye moves, what content is overlooked and virtually any other gaze-related question. \bigskip

Think of all the ways that you will use your eyes today. You might drive a car. You might read a magazine. You might surf the internet or play a video game or complete a training exercise or watch a movie or look at pictures on your mobile device. These are the applications of eye tracking. With very few exceptions, anything with a visual component can be eye tracked. We use our eyes almost constantly, and understanding how we use them has become an extremely important consideration in research and design. \bigskip

Now days, the automotive and medical industries have applied eye tracking technology to make us safer. The fields of advertising, entertainment, packaging and web design have all benefited significantly from studying the visual behaviour of the consumer. Research with special populations has generated exciting breakthroughs in psychology. Every day, as eye tracking is used in creative new ways, the list of applications grows. \bigskip

Today's Eye-tracking algorithms use two main approaches: feature-based and model-based approaches. \bigskip \\
Feature-based approaches detect and localize image features related to position of the eye. There is an important property among all feature-based algorithm which is criteria (e.g. a threshold) needed to decide when a certain feature is present or not. Usually the determination of the criteria or the threshold left to the user. For example on the feature-based algorithm is Starburst because it depends on the corneal reflection. \bigskip \cite{starburst}

On the other hand, model-based approaches don't explicitly detect features but rather find the best fitting model that is consistent with the image. For example Pupil Detection algorithm is model-based approach. However, model-based approaches require searching a complex parameter space that can be fraught with local minima and thus cannot be used without a good initial guess of the model parameters. The gain in accuracy of a model-based approach is obtained at a significant cost in terms of computational speed and flexibility but it is so promise for real-time performance. \bigskip \cite{starburst}

 


\subsubsection{Starburst}\index{Starburst}
Starburst is a robust algorithm for video based eye tracking. We started with implementing Starburst Algorithm. This algorithm is more accurate than pure feature-based approaches yet is significantly less time consuming than pure model-based approaches. A validation study shows that the technique can reliably estimate eye position with an accuracy of approximately one degree of visual angle even in the presence of significant image noise. \bigskip

The use of eye tracking has significant potential to enhance the quality of everyday human-computer interfaces. Two types of human-computer interfaces utilize eye-movement measures active and passive interfaces. Active interfaces allow users to explicitly control the interface through the use of eye movements. For example, eye-typing applications allow the user to look at keys on a virtual keyboard to type instead of manually pressing keys as with a traditional keyboard. Similarly, systems have been designed that allow users to control the mouse pointer with their eyes in a way that can support, for example, the drawing of pictures. Active interfaces that allow users with movement disabilities to interact with computers may also be helpful for healthy users by speeding icon selection in graphical user interfaces or object selection in virtual reality. Passive interfaces, on the other hand, monitor the user's eye movements and use this information to adapt some aspect of the display. For example, in video transmission and virtual-reality applications, gaze contingent variable-resolution display techniques present a high level of detail at the point of gaze while sacrificing level of detail in the periphery where the absence of detail is not distracting. While eye tracking has been deployed in a number of research systems and to some smaller degree consumer products, eye tracking has not reached its full potential.

\subsubsection{Robust Realtime Pupil Tracking}\index{Robust Realtime Pupil Tracking}
In this paper, they present a real-time dark-pupil tracking algorithm designed for low-cost head-mounted active-IR hardware. Their algorithm is robust to highly eccentric pupil ellipses and partial obstructions from eyelashes, making it suitable for use with cameras mounted close to the eye. It first computes a fast initial approximation of the pupil position, and performs a novel RANSAC based ellipse fitting to robustly refine this approximation.

\subsubsection{Pupil}\index{Pupil}
Pupil is an open source platform for pervasive eye tracking and mobile gazed interaction. In this paper, they argue that affordability does not necessarily align with accessibility. They define accessible eye tracking platforms to have the following qualities: open source components, modular hardware and software design, comprehensive documentation, user support, affordable price, and flexibility for future changes. \bigskip

They have developed Pupil, a mobile eye tracking headset and an open source software framework, as an accessible, affordable, and extensible tool for pervasive eye tracking research. They explain the design motivation of the system, provide an in depth technical description of both hardware and software, and provide an analysis of accuracy and performance of the system.
\bigskip


\subsubsection{Eye-based head gestures}
	Gestures are commonly used for interaction and are used to signify a particular command. For Eyes there are two types of gestures, namely eye and gaze gestures. Eye gestures such as wink and blinks. But actually eye gestures specially repetitive blinking for long-term use may create a feeling of nervous eye muscles.\cite{drewes2010eye} Gaze gestures, on the other hand, are definable patterns of eye movements performed within a limited time interval.\cite{istance2010designing} But simple gaze gestures are not distinguishable from natural eye patterns and make unintended interaction similar to the Midas-
touch problem. Complex gaze gestures consist of several simple gaze gestures are therefore needed for robust results.\bigskip

	Head nods and shakes are widely used in our daily conversation as a gesture to fulfill a semantic function and as conversational feedback (e.g., nodding instead of saying yes). \cite{darwin1998expression}
	\bigskip
	
	So that is a novel for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures.\cite{mardanbegi2012eye}

\subsubsection{Eye Tracking in Human-Computer Interaction and Usability Research}

Eye-movement tracking is a method that is increasingly being employed to study usability issues in HCI contexts. So, it is a technique whereby an individual’s eye movements are measured so that the researcher knows both where a person is looking at any given time and the sequence in which their eyes are shifting from one location to another. Tracking people’s eye movements can help HCI researchers understand visual and display-based information processing and the factors that may impact upon the usability of system
interfaces. \bigskip
 
	In this way, eye-movement recordings can provide an objective source of interface-evaluation data that can inform the design of improved interfaces. Eye movements can also be captured and used as control signals to enable people to interact with interfaces directly without the need for mouse or keyboard input, which can be a major advantage for certain populations of users such as disabled individuals. Also Eye tracking can be used in usability testing.\cite{poole2006eye}

\subsection{Gesture Recognition}\index{Gesture Recognition}
Traditional human-computer interaction devices such as the keyboard and mouse became ineffective for an effective interaction with the virtual environment applications because the 3D applications need a new interaction device. An efficient human interaction with the modern virtual environments requires more natural devices. Among them the “Hand Gesture” human-computer interaction modality has recently become of major interest. The main objective of gesture recognition research is to build a system which can recognize human gestures and utilize them to control an application. Hand gestures are a collection of movements of the hand and arm that vary from the static posture of pointing at something to the dynamic ones used to communicate with others. Recognizing these hand movements needs modeling them in the
spatial and temporal domains. Hand posture is the static structure of the hand while its dynamic movement is called hand gesture and both are particularly crucial for human-computer interaction. The methods used for understanding these structures and movements are among the most classifying researches that still in progress.
\bigskip

\subsubsection{Static Gesture Recognition}\index{Static Gesture Recognition}

Many approaches have been developed for gesture recognition. Many researchers \cite{yrel7} and \cite{yrel16} have tried with different instruments and equipment to measure hand movements like gloves, sensors or wires, but in these techniques user have to wear the device which doesn’t make sense in practical use. So people thought about a way of contactless gesture recognition that could be considered as a research area in Machine Vision or Computer Vision and which would be as natural as human to human interaction.
\bigskip

In \cite{yrel1},\cite{relatedsg1} and \cite{relatedsg2}  approaches are divided based on the method used in it not on how it is treating the image. Approaches are divided into:
\begin{itemize}
\item Appearance Based Approaches.
\item Model Based Approaches.
\item Soft Computing Approaches.
\end{itemize}

\bigskip

Appearance Based Approaches:
\begin{itemize}

\item Based on detecting fingertips to construct the hand.
\item Skin segmentation using Gaussian Model.
\item Gaussian filters and freeman's algorithm to detect fingertips.
\item Kalman filter for finding fingertips and their correct positions in a recursive manner.
\end{itemize}

\bigskip

Model Based Approaches:
\begin{itemize}
\item Using histogram for calculating probability for skin color.
\item Genetic Algorithms approach.
\item RGB and HSI color space model based algorithm for skin detection.	
\end{itemize}

\bigskip
Soft Computing Approaches:
\begin{itemize}
\item Artificial Neural Networks.
\item Fuzzy Logic.
\item Genetic Algorithm.
\end{itemize}

\bigskip



There have been a number of research efforts on appearance based method in recent years. Freeman and Weissman recognized gestures for television control using normalized correlation \cite{relatedsg3}. This technique is efficient but may be sensitive to different users, deformations of the pose and changes in scale, and background. Lars and Lindberg used scale-space color features to recognize hand gestures \cite{relatedsg4}. In their method, gesture recognition method is based on feature detection and user independent while the authors showed real-time application only under uniform backgrounds.
\bigskip

Yikai Fang, Kongqiao Wang proposed "A REAL-TIME HAND GESTURE RECOGNITION METHOD" \cite{relatedsg5} inspired by Lars and Lindberg’s works and HandVu, they present a robust real-time gesture recognition method. The main idea is to segment hand with color and motion cues generated by detection and tracking. Then scale-space feature detection method is used to recognize hand gestures. They partitioned their work into 3 modules as follows:\bigskip
\begin{enumerate}
\item  Firstly hand detection with Adaboost is used to trigger tracking and recognition.
\item Then Adaptive hand segmentation is executed during detection and tracking with motion and color cues.
\item  Finally, scale-space features detection is applied to find palm-like and finger-like structures. Hand gesture type is determined by palm-finger configuration.
\end{enumerate}
\bigskip

Serra, Giuseppe, Lorenzo and Benedetti proposed the paper “Hand Segmentation for Gesture Recognition in EGO-Vision” \cite{relatedsg6}. In their paper, they are interested in exploring the usage of ego-vison devices for cultural heritage domain: the museum experience, for example, could be enhanced by developing innovative human-machine interfaces such as new kinds of self-guided tour that can integrate information from the local environment, Web and social medias.
They divided their work into two main contributions:
\bigskip

\begin{itemize}
\item define a novel hand segmentation algorithm, uses a super-pixel features, and integrate not only illumination invariance but also temporal and spatial consistency improves the state-of-the-art results in the publicly available EDSH dataset.
\item Develop of a gesture recognition algorithm based on Exemplar SVM technique that, even with a few positive samples, permits to reach competitive results.
\end{itemize}
\bigskip

Our Work is summarized by implementing the following approaches for static gestures as detailed in chapter \ref{ch_gesture}:
\begin{itemize}
\item Gridding Algorithm: A simple idea for Recognizing hand gestures based on their binary representation as described in \cite{paper1sb}.
\bigskip

\item Predefined Features Extraction: More complicated approach. where, hand gesture is represented by a collection of features that summarize its properties as described in \cite{paper2sT}.
\begin{enumerate}
\item Mean an variance of hand grey image.
\item Hand area and perimeter.
\item Hand orientation.
\item Orientation Histogram: Avoid sensitivity to lighting conditions. 
\item Radial signature.
\end{enumerate}
\bigskip

\item Bag-Of-Features: SIFT features are extracted from images, then quantized using K-means clustering in order to be classified later.
\end{itemize}
\bigskip

In all static approaches, Classifiers are trained by the extracted Features. 
We used three classification techniques for evaluation (K-NN, Naive-Bayes, and SVM). Then, we agreed that SVM is most suitable technique for our gestures as detailed in chapter \ref{ch_gr_eval}.

\subsubsection{Dynamic Gestures Recognition} \index{Dynamic Gestures Recognition}
Hidden Markov Models (HMMs) are used in \cite{hoda} for continuous gesture recognition. HMMs are statistical models capable of modeling spatio-temporal patterns where the same gesture can differ in shape and duration.
\bigskip

This approach assumes that the hand has been detected and its articulated motion is tracked. Although they have delivered promising results, the robustness of these approaches is dependent on the prior success of (frequently challenging) hand detection and motion tracking. In addition, it is both data intensive and computationally difficult to train these models before they can be applied in recognition.
\bigskip

The approach proposed by Xiaohui, Gang, in \cite{dynamic2} visual representation for hand motions based on the motion divergence fields, which can be normalized to gray-scale images. Salient regions such as Maximum Stable Extremal Regions(MSER) are then detected on the motion divergence maps.From each detected region, a local descriptor is extracted to capture local motion patterns.
\bigskip

 For our HCI purposes, we need relatively less computational methods suitable for applications on devices with limited capabilities. So, in our project, the second approach that estimate motion direction\cite{dynamic2} is adapted for recognizing dynamic gestures as described in chapter \ref{ch_gesture}.
 
 